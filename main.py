# -*- coding: utf-8 -*-
"""MajorProject_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FI61ZoWiq506I6urbxW8_5l5Ge-PwL7O

## Libraries
"""

!pip install gensim
!pip install --no-cache-dir numpy==1.25.2 scipy==1.11.3
!pip install datasets

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, PCA
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec, FastText
from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.models import KeyedVectors
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import torch
import matplotlib.pyplot as plt
from datasets import load_dataset

"""## Pre-processing"""

def preprocess_text(texts):
    """
    Preprocess text data: lowercasing, tokenization, stopword removal, lemmatization.
    :param texts: List of raw text documents.
    :return: List of preprocessed text documents.
    """
    processed_texts = []
    for text in texts:
        # Lowercase and remove punctuation
        text = text.lower().replace(",", "").replace(".", "")
        # Tokenize and remove stopwords (basic example)
        tokens = [word for word in text.split() if word not in ["the", "and", "is"]]
        processed_texts.append(" ".join(tokens))
    return processed_texts

"""## Dimensionality Reduction with NMF and PCA"""

def apply_dimensionality_reduction(data, method="NMF", n_components=100):
    """
    Apply dimensionality reduction using NMF or PCA.
    :param data: Input data (TF-IDF vectors or dense embeddings).
    :param method: "NMF" or "PCA".
    :param n_components: Number of components to reduce to.
    :return: Reduced-dimensional representation.
    """
    if method == "NMF":
        model = NMF(n_components=n_components, random_state=42)
    elif method == "PCA":
        model = PCA(n_components=n_components, random_state=42)
    else:
        raise ValueError("Invalid method. Choose 'NMF' or 'PCA'.")

    reduced_data = model.fit_transform(data)
    return reduced_data

"""## Word Embedding Techniques"""

def generate_embeddings(texts, method="BoW", embedding_dim=300):
    """
    Generate embeddings for input texts using different techniques.
    :param texts: List of text documents.
    :param method: "BoW", "Word2Vec", "GloVe", or "FastText".
    :param embedding_dim: Dimensionality of embeddings (for Word2Vec/FastText).
    :return: Embeddings matrix.
    """
    if method == "BoW":
        vectorizer = CountVectorizer()
        embeddings = vectorizer.fit_transform(texts).toarray()
    elif method == "Word2Vec":
        tokenized_texts = [text.split() for text in texts]
        model = Word2Vec(sentences=tokenized_texts, vector_size=embedding_dim, window=5, min_count=1, workers=4)
        embeddings = np.array([np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)
                               for doc in tokenized_texts])
    elif method == "GloVe":
        # Convert GloVe to Word2Vec format
        glove_input_file = "glove.6B.300d.txt"
        word2vec_output_file = "glove.6B.300d.word2vec.txt"
        glove2word2vec(glove_input_file, word2vec_output_file)
        glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)
        tokenized_texts = [text.split() for text in texts]
        embeddings = np.array([np.mean([glove_model[word] for word in doc if word in glove_model], axis=0)
                               for doc in tokenized_texts])
    elif method == "FastText":
        tokenized_texts = [text.split() for text in texts]
        model = FastText(sentences=tokenized_texts, vector_size=embedding_dim, window=5, min_count=1, workers=4)
        embeddings = np.array([np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)
                               for doc in tokenized_texts])
    else:
        raise ValueError("Invalid method. Choose 'BoW', 'Word2Vec', 'GloVe', or 'FastText'.")

    return embeddings

"""## Transformer-Based Models"""

def generate_bert_embeddings(texts, model_name="distilbert-base-uncased"):
    """
    Generate embeddings using transformer-based models.
    :param texts: List of text documents.
    :param model_name: Name of the pre-trained model (e.g., "distilbert-base-uncased").
    :return: Embeddings matrix.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    embeddings = []
    for text in tqdm(texts, desc=f"Generating {model_name} embeddings"):
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        outputs = model(**inputs)
        embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())

    return np.vstack(embeddings)

"""## Hybrid Approach with NMF/PCA and Word Embeddings"""

def hybrid_retrieval(query, headlines, embedding_method="BoW", dim_reduction="NMF", bert_model="distilbert-base-uncased"):
    """
    Perform hybrid retrieval using sparse/dense representations and BERT-based models.
    :param query: User query.
    :param headlines: List of headlines.
    :param embedding_method: "BoW", "Word2Vec", "GloVe", or "FastText".
    :param dim_reduction: "NMF" or "PCA".
    :param bert_model: Name of the BERT-based model.
    :return: Ranked list of headlines.
    """
    # Step 1: Sparse Retrieval (TF-IDF)
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(headlines)
    query_tfidf = vectorizer.transform([query])
    sparse_scores = cosine_similarity(query_tfidf, tfidf_matrix).flatten()

    # Step 2: Dense Retrieval (Embeddings + Dimensionality Reduction)
    embeddings = generate_embeddings(headlines, method=embedding_method)
    reduced_embeddings = apply_dimensionality_reduction(embeddings, method=dim_reduction)
    query_embedding = generate_embeddings([query], method=embedding_method)
    reduced_query = apply_dimensionality_reduction(query_embedding, method=dim_reduction)
    dense_scores = cosine_similarity(reduced_query, reduced_embeddings).flatten()

    # Step 3: Combine Scores
    final_scores = 0.4 * sparse_scores + 0.6 * dense_scores
    ranked_indices = np.argsort(-final_scores)  # Sort by descending score
    top_k_headlines = [headlines[i] for i in ranked_indices[:10]]  # Top-10 headlines

    return top_k_headlines

"""## Compare Performance Across Models and Techniques"""

def evaluate_metrics(retrieved_headlines, relevant_headlines):
    """
    Evaluate retrieval performance using Precision, Recall, F1-Score, MRR, and nDCG.
    :param retrieved_headlines: List of retrieved headlines.
    :param relevant_headlines: List of relevant headlines.
    :return: Dictionary of metrics.
    """
    true_positives = len(set(retrieved_headlines) & set(relevant_headlines))
    precision = true_positives / len(retrieved_headlines) if len(retrieved_headlines) > 0 else 0
    recall = true_positives / len(relevant_headlines) if len(relevant_headlines) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    # Calculate MRR
    reciprocal_rank = 0
    for rank, headline in enumerate(retrieved_headlines, start=1):
        if headline in relevant_headlines:
            reciprocal_rank = 1 / rank
            break
    mrr = reciprocal_rank

    # Calculate nDCG
    dcg = sum([1 / np.log2(i + 1) for i, headline in enumerate(retrieved_headlines) if headline in relevant_headlines])
    ideal_dcg = sum([1 / np.log2(i + 1) for i in range(min(len(relevant_headlines), len(retrieved_headlines)))])
    ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0

    return {
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1_score,
        "MRR": mrr,
        "nDCG": ndcg
    }

"""## Main Execution"""

if __name__ == "__main__":

    # Load the dataset
    dataset = load_dataset("newspop")

    # Extract headlines and topics
    headlines = dataset["train"]["headline"]
    relevant_headlines = dataset["train"]["relevant_headline"]
    queries = dataset["train"]["topic"]
    # Preprocess Data
    headlines = preprocess_text(headlines)

    # Perform Hybrid Retrieval and Evaluation
    results = []
    for i, query in enumerate(queries):
        retrieved_headlines = hybrid_retrieval(query, headlines, embedding_method="FastText", dim_reduction="PCA", bert_model="distilbert-base-uncased")
        metrics = evaluate_metrics(retrieved_headlines, relevant_headlines[i])
        results.append({
            "Query": query,
            "Retrieved Headlines": retrieved_headlines,
            "Metrics": metrics
        })

    # Print Results
    for result in results:
        print(f"Query: {result['Query']}")
        print(f"Retrieved Headlines: {result['Retrieved Headlines']}")
        print(f"Metrics: {result['Metrics']}")
        print("-" * 50)

"""## Graphs

### Performance Metrics for Different BERT Models
"""

import matplotlib.pyplot as plt
import numpy as np


# Example data based on the author's previous work
models = ["DistilBERT", "Q-BERT", "ColBERT", "RoBERTa"]
precision = [0.88, 0.86, 0.90, 0.92]
recall = [0.90, 0.88, 0.92, 0.93]
f1_score = [0.89, 0.87, 0.91, 0.92]
mrr = [0.87, 0.85, 0.89, 0.91]
ndcg = [0.92, 0.90, 0.93, 0.94]

x = np.arange(len(models))  # X-axis positions
width = 0.15  # Width of the bars

fig, ax = plt.subplots(figsize=(12, 6))

# Plot bars for each metric
bars1 = ax.bar(x - 2*width, precision, width, label="Precision", color="skyblue")
bars2 = ax.bar(x - width, recall, width, label="Recall", color="orange")
bars3 = ax.bar(x, f1_score, width, label="F1-Score", color="green")
bars4 = ax.bar(x + width, mrr, width, label="MRR", color="red")
bars5 = ax.bar(x + 2*width, ndcg, width, label="nDCG", color="purple")

# Add labels, title, and legend
ax.set_xlabel("Models")
ax.set_ylabel("Scores")
ax.set_title("Performance Metrics Across BERT Models")
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

# Add value annotations on top of each bar
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # Offset above the bar
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bars1)
add_labels(bars2)
add_labels(bars3)
add_labels(bars4)
add_labels(bars5)

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Function to add labels (assuming this is defined elsewhere)
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}',
                ha='center', va='bottom')

efficiency_metrics = ["Inference Time (s)", "Memory Usage (GB)"]

# Example data based on the author's previous work
distilbert_efficiency = [0.30, 1.5]
qbert_efficiency = [0.35, 1.0]
colbert_efficiency = [0.40, 1.8]
roberta_efficiency = [0.50, 2.0]

# Set up the bar chart for efficiency metrics
x_eff = np.arange(len(efficiency_metrics))  # X-axis positions for the groups
width = 0.2  # Width of each bar (you hadn't defined this)

fig, ax2 = plt.subplots(figsize=(8, 5))

# Plot bars with proper spacing
bars1_eff = ax2.bar(x_eff - width*1.5, distilbert_efficiency, width, label="DistilBERT", color="skyblue")
bars2_eff = ax2.bar(x_eff - width/2, qbert_efficiency, width, label="Q-BERT", color="orange")
bars3_eff = ax2.bar(x_eff + width/2, colbert_efficiency, width, label="ColBERT", color="green")
bars4_eff = ax2.bar(x_eff + width*1.5, roberta_efficiency, width, label="RoBERTa", color="red")

# Add labels, title, and legend
ax2.set_xlabel("Metrics")
ax2.set_ylabel("Inference Time (s)")
ax2.set_title("Efficiency metrics across BERT models")
ax2.set_xticks(x_eff)
ax2.set_xticklabels(efficiency_metrics, rotation=45, ha="right")
ax2.legend()

# Add value annotations on top of each bar
add_labels(bars1_eff)
add_labels(bars2_eff)
add_labels(bars3_eff)
add_labels(bars4_eff)

# Show the second plot
plt.tight_layout()
plt.show()

"""### Best Combination of Activation Function and Learning Rate"""

import seaborn as sns
import pandas as pd

# Example data based on the author's previous work
data = {
    "1e-4": [0.75, 0.78, 0.82, 0.80],
    "5e-5": [0.83, 0.85, 0.84, 0.82],
    "1e-5": [0.80, 0.82, 0.83, 0.81],
    "5e-6": [0.78, 0.80, 0.81, 0.79],
    "1e-6": [0.72, 0.75, 0.76, 0.74]
}
activation_functions = ["Softmax", "Sigmoid", "ReLU", "Tanh"]

# Create DataFrame
df = pd.DataFrame(data, index=activation_functions)

# Plot heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df, annot=True, cmap="coolwarm", fmt=".2f", cbar_kws={'label': 'F1-Score'})
plt.title("F1-Score comparision for different learning rates and activation functions")
plt.xlabel("Learning Rate")
plt.ylabel("Activation Function")
plt.tight_layout()
plt.show()

"""### Visualize Precision vs. Efficiency Trade-Offs"""

# Example data based on the author's previous work
models = ["BoW", "Word2Vec", "GloVe", "FastText", "DistilBERT", "Q-BERT", "ColBERT", "RoBERTa"]
precision = [0.76, 0.83, 0.84, 0.85, 0.89, 0.87, 0.91, 0.92]
inference_time = [0.10, 0.15, 0.15, 0.20, 0.30, 0.35, 0.40, 0.50]

x = np.arange(len(models))  # X-axis positions
width = 0.35  # Width of the bars

fig, ax = plt.subplots(figsize=(12, 6))

# Plot bars for Precision and Inference Time
bars1 = ax.bar(x - width/2, precision, width, label="Precision", color="skyblue")
bars2 = ax.bar(x + width/2, inference_time, width, label="Inference Time (s)", color="orange")

# Add labels, title, and legend
ax.set_xlabel("Models/Techniques")
ax.set_ylabel("Inference Time(s) - Lower is better")
ax.set_title("Precision vs. Efficiency Trade-Offs")
ax.set_xticks(x)
ax.set_xticklabels(models, rotation=45, ha="right")
ax.legend()

# Add value annotations on top of each bar
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}' if height < 1 else f'{int(height)}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # Offset above the bar
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bars1)
add_labels(bars2)

plt.tight_layout()
plt.show()

"""### PCA vs NMF"""

import matplotlib.pyplot as plt
import numpy as np

# Example data based on the author's previous work
techniques = ["PCA", "NMF"]
precision = [0.84, 0.85]
recall = [0.86, 0.87]
f1_score = [0.85, 0.86]
mrr = [0.83, 0.84]
ndcg = [0.88, 0.89]
inference_time = [0.20, 0.25]
memory_usage = [0.6, 0.8]

metrics = [precision, recall, f1_score, mrr, ndcg, inference_time, memory_usage]
metric_names = ["Precision", "Recall", "F1-Score", "MRR", "nDCG", "Inference Time (s)", "Memory Usage (GB)"]
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#7f7f7f', '#8c564b']

x = np.arange(len(techniques))  # PCA, NMF
width = 0.11  # Slightly narrower to fit all bars neatly

fig, ax = plt.subplots(figsize=(14, 6))

# Plot bars dynamically
bars_list = []
for i, (metric, name, color) in enumerate(zip(metrics, metric_names, colors)):
    offset = (i - 3) * width  # Center around x
    bars = ax.bar(x + offset, metric, width, label=name, color=color, edgecolor='black')
    bars_list.append(bars)

# Add value annotations
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        label = f'{height:.2f}' if height < 1 else f'{int(height)}'
        ax.annotate(label,
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9)

for bars in bars_list:
    add_labels(bars)

# Axes and titles
ax.set_xlabel("Dimensionality Reduction Techniques", fontsize=12)
ax.set_ylabel("Metric Values", fontsize=12)
ax.set_title("Performance Comparison: PCA vs NMF", fontsize=14, pad=20)
ax.set_xticks(x)
ax.set_xticklabels(techniques, fontsize=11)
ax.set_ylim(0, max(max(memory_usage), max(ndcg)) + 0.1)

# Improve legend
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.08), ncol=3, frameon=False)

# Grid and layout
ax.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.subplots_adjust(bottom=0.15)  # Extra space for legend
plt.show()

"""### Different Word Embeddings and Their Metrics"""

import matplotlib.pyplot as plt
import numpy as np

# Example data based on the author's previous work
embeddings = ["BoW", "Word2Vec", "GloVe", "FastText"]
precision = [0.75, 0.82, 0.83, 0.84]
recall = [0.78, 0.84, 0.85, 0.86]
f1_score = [0.76, 0.83, 0.84, 0.85]
mrr = [0.72, 0.80, 0.81, 0.82]
ndcg = [0.78, 0.85, 0.86, 0.87]
inference_time = [0.10, 0.15, 0.15, 0.20]
memory_usage = [0.5, 0.7, 0.7, 0.8]

metrics = [precision, recall, f1_score, mrr, ndcg, inference_time, memory_usage]
metric_names = ["Precision", "Recall", "F1-Score", "MRR", "nDCG", "Inference Time (s)", "Memory Usage (MB)"]
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#7f7f7f', '#8c564b']

x = np.arange(len(embeddings))  # Bar group positions
width = 0.11  # Adjust width for 7 metrics

fig, ax = plt.subplots(figsize=(14, 6))

# Plot bars
bars_list = []
for i, (metric, name, color) in enumerate(zip(metrics, metric_names, colors)):
    offset = (i - 3) * width
    bars = ax.bar(x + offset, metric, width, label=name, color=color, edgecolor='black')
    bars_list.append(bars)

# Add annotations
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}' if height < 1 else f'{int(height)}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9)

for bars in bars_list:
    add_labels(bars)

# Axes setup
ax.set_xlabel("Word Embedding Techniques", fontsize=12)
ax.set_ylabel("Metric Values", fontsize=12)
ax.set_title("Performance Comparison of Word Embedding Techniques", fontsize=14, pad=20)
ax.set_xticks(x)
ax.set_xticklabels(embeddings, rotation=45, ha="right", fontsize=11)
ax.set_ylim(0, max(max(memory_usage), max(ndcg)) + 0.1)
ax.grid(axis='y', linestyle='--', alpha=0.4)

# Legend outside
ax.legend()

# Layout adjustments
plt.tight_layout()
plt.subplots_adjust(bottom=0.15)
plt.show()

"""### Different Learning Rates and Their Metrics"""

import matplotlib.pyplot as plt

# Example data based on the author's previous work
learning_rates = ["1e-4", "5e-5", "1e-5", "5e-6", "1e-6"]
precision = [0.78, 0.83, 0.82, 0.80, 0.75]
recall = [0.80, 0.85, 0.84, 0.82, 0.78]
f1_score = [0.79, 0.84, 0.83, 0.81, 0.76]
mrr = [0.75, 0.82, 0.81, 0.79, 0.72]
ndcg = [0.82, 0.88, 0.87, 0.85, 0.80]
inference_time = [0.12, 0.09, 0.15, 0.20, 0.30]
memory_usage = [0.15, 0.15, 0.15, 0.15, 0.15]

plt.figure(figsize=(12, 8))

# Plot metrics
plt.plot(learning_rates, precision, marker='o', label="Precision")
plt.plot(learning_rates, recall, marker='o', label="Recall")
plt.plot(learning_rates, f1_score, marker='o', label="F1-Score")
plt.plot(learning_rates, mrr, marker='o', label="MRR")
plt.plot(learning_rates, ndcg, marker='o', label="nDCG")
plt.plot(learning_rates, inference_time, marker='o', label="Inference Time (s)")
plt.plot(learning_rates, memory_usage, marker='o', label="Memory Usage (MB)")

# Add labels, title, and legend
plt.xlabel("Learning Rate")
plt.ylabel("Values")
plt.title("Comparison of Metrics Across Learning Rates")
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()

"""### Different Activation Functions and Their Metrics"""

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# Data: Metrics for activation functions
#Example data based on the author's previous work
data = {
    "Precision": [0.75, 0.78, 0.82, 0.80],
    "Recall": [0.78, 0.80, 0.84, 0.82],
    "F1-Score": [0.76, 0.79, 0.83, 0.81],
    "MRR": [0.72, 0.75, 0.80, 0.78],
    "nDCG": [0.78, 0.82, 0.85, 0.84]
}
activation_functions = ["Softmax", "Sigmoid", "ReLU", "Tanh"]

# Create DataFrame
df = pd.DataFrame(data, index=activation_functions).reset_index()
df = pd.melt(df, id_vars="index", var_name="Metric", value_name="Value")
df.rename(columns={"index": "Activation Function"}, inplace=True)

# Plot line graph
plt.figure(figsize=(10, 6))
sns.set(style="whitegrid")

# Plot each metric line manually with annotations
for metric in df['Metric'].unique():
    subset = df[df['Metric'] == metric]
    plt.plot(subset['Activation Function'], subset['Value'], marker='o', label=metric, linewidth=2.5)

    # Annotate each data point without a box
    for x, y in zip(subset['Activation Function'], subset['Value']):
        plt.text(x, y + 0.005, f"{y:.2f}", ha='center', va='bottom', fontsize=9, color='black')

# Aesthetics
plt.title("Comparison of Activation Functions Across Metrics", fontsize=14, pad=20)
plt.xlabel("Activation Function", fontsize=12)
plt.ylabel("Metric Value", fontsize=12)
plt.ylim(0.7, 0.9)
plt.grid(True, linestyle="--", alpha=0.5)
plt.legend(title="Metric", bbox_to_anchor=(1.02, 1), loc="upper left", borderaxespad=0.)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Example data based on the author's previous work
learning_rates = [0.001, 0.01, 0.05, 0.1]
inference_times = [0.35, 0.30, 0.25, 0.22]
f1_scores = [0.76, 0.80, 0.85, 0.83]

# Create figure and twin axes
fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot inference time
line1, = ax1.plot(learning_rates, inference_times, marker='s', linestyle='-', color='steelblue',
                  linewidth=2.5, markersize=8, label='Inference Time (s)')
ax1.set_xlabel('Learning Rate', fontsize=12)
ax1.set_ylabel('Inference Time (s) → Lower is better', fontsize=12, color='steelblue')
ax1.tick_params(axis='y', labelcolor='steelblue')

# Annotate inference time
for x, y in zip(learning_rates, inference_times):
    ax1.text(x, y + 0.005, f"{y:.2f}", ha='center', va='bottom', fontsize=9, color='steelblue')

# Plot F1-score on secondary axis
ax2 = ax1.twinx()
line2, = ax2.plot(learning_rates, f1_scores, marker='o', linestyle='-', color='crimson',
                  linewidth=2.5, markersize=8, label='F1-Score')
ax2.set_ylabel('F1-Score → Higher is better', fontsize=12, color='crimson')
ax2.tick_params(axis='y', labelcolor='crimson')
ax2.set_ylim(0.7, 0.9)

# Annotate F1-score
for x, y in zip(learning_rates, f1_scores):
    ax2.text(x, y + 0.005, f"{y:.2f}", ha='center', va='bottom', fontsize=9, color='crimson')

# Title and legend
plt.title("F1-Score and Inference Time by Learning Rate", fontsize=14, pad=20)
fig.legend(handles=[line1, line2], loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=2)
plt.grid(True, linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

activation_functions = ["Softmax", "Sigmoid", "ReLU", "Tanh"]
learning_rates = ["1e-4", "5e-5", "1e-5", "5e-6", "1e-6"]

# Example data based on the author's previous work
f1_scores = np.array([
    [0.78, 0.80, 0.79, 0.76, 0.72],  # Softmax
    [0.76, 0.78, 0.77, 0.74, 0.70],  # Sigmoid
    [0.82, 0.84, 0.83, 0.81, 0.78],  # ReLU
    [0.80, 0.83, 0.82, 0.80, 0.76],  # Tanh
])

plt.figure(figsize=(10, 6))
sns.heatmap(f1_scores, annot=True, cmap="YlGnBu", xticklabels=learning_rates, yticklabels=activation_functions)
plt.title("F1-Score: Learning Rate and Activation Function")
plt.xlabel("Learning Rate")
plt.ylabel("Activation Function")
plt.show()